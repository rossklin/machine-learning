* todo
** Agent tstats should be serialized
** [done] For some reason, team evaluator tag was missing from save files, and dim was uninitialized
Missing from team evaluator default constructor
** [done] Why does mutant generator get same complexity in every case?
Population converged to mostly one individual
** [done] Why are most children generated with ~2x as high complexity as parents, is mate score reversed or something?
Trial and error searching for required inputs caused higher complexities
** [done] Create a run-id instead of manually managing data
** [done] Specific game data needs info about epoch, rank, opponent and checkpoints
** Fine tune inner optimization parameters
** Try simulated annealing 
** [done] Add reference games vs oldest agents in retirement
** [done] All agents are dropped under condition: success > 0.5
[disabled] Protected ages are no longer valid since age increases when training
[fixed logic] Agents are validated "successfull" at end of epoch, but dropped as "not successfull" after tournament
[fixed] No agents ever qualify as progressors

This means no agents ever survive to next epoch
** [done] Use analyitcal derivative in optimization
** [done] Add deserialization of retirement home
** [done] Use agent stats to filter agents in prepared_player and in epoch post processing
** [done] Add a retirement home
** [done] Define required inputs so agent generator can avoid testing obvious failures
** [done] refbot must be analyzed and fixed
** [done] supervision must be based on games between teachers, not teacher vs candidate
** [done] Ancestors and parents must be saved when serializing agents (updated serialize but not deserialize)
** [done] Check if protection is ignored? Seems to always make 12 + 11 new agents?
** [done] Check if new agents have a score advantage, or if tournament training hurts agents, since best agent always seems to be 1-2 years old
** [done] preparation should use parent(s) as supervisor(s) when available
** [done] preparation should implement a scheme to mix supervision with practice and reduce supervision over time
** [done] preparation must set appropriate exploration rates
** [done] tournament should be split into practice rounds with no scoring and real rounds
** [done] tournament should set the same exploration rates for all players, high in practice rounds and low in real rounds
* ideas
** Template structure
*** Must create an abstract agent class so refbot and agent can run in the same game
*** Thus, we can go back to using a generic agent_ptr instead of all the local defs
** tree evaluator
*** store derivative of output with respect to each sub-tree result in each iteration
    Sub-trees with high variance or just high derivatives should be
    more relevant, now you can prefer trimming less relevant sub-trees and
    use more relevant sub-trees in mating.
** evolution algorithm
*** avoid keeping a large share of agents from the same "family"
